---
---

@string{aps = {American Physical Society,}}

@article{shevchenko2019scaling,
  abbr={arXiv},
  title={Scaling Matters in Deep Structured-Prediction Models},
  author={Shevchenko, Alexander and Osokin, Anton},
  journal={arXiv preprint arXiv:1902.11088},
  year={2019},
  selected={false},
  abstract={Deep structured-prediction energy-based models combine the expressive power of learned representations and the ability of embedding knowledge about the task at hand into the system. A common way to learn parameters of such models consists in a multistage procedure where different combinations of components are trained at different stages. The joint end-to-end training of the whole system is then done as the last fine-tuning stage. This multistage approach is time-consuming and cumbersome as it requires multiple runs until convergence and multiple rounds of hyperparameter tuning. From this point of view, it is beneficial to start the joint training procedure from the beginning. However, such approaches often unexpectedly fail and deliver results worse than the multistage ones. In this paper, we hypothesize that one reason for joint training of deep energy-based models to fail is the incorrect relative normalization of different components in the energy function. We propose online and offline scaling algorithms that fix the joint training and demonstrate their efficacy on three different tasks.},
  pdf={https://arxiv.org/pdf/1902.11088.pdf}
}

@article{shevchenko2022fundamental,
  abstract={Autoencoders are a popular model in many branches of machine learning and lossy data compression. However, their fundamental limits, the performance of gradient methods and the features learnt during optimization remain poorly understood, even in the two-layer setting. In fact, earlier work has considered either linear autoencoders or specific training regimes (leading to vanishing or diverging compression rates). Our paper addresses this gap by focusing on non-linear two-layer autoencoders trained in the challenging proportional regime in which the input dimension scales linearly with the size of the representation. Our results characterize the minimizers of the population risk, and show that such minimizers are achieved by gradient methods; their structure is also unveiled, thus leading to a concise description of the features obtained via training. For the special case of a sign activation function, our analysis establishes the fundamental limits for the lossy compression of Gaussian sources via (shallow) autoencoders. Finally, while the results are proved for Gaussian data, numerical simulations on standard datasets display the universality of the theoretical predictions.},
  abbr={arXiv},
  title={Fundamental Limits of Two-layer Autoencoders, and Achieving Them with Gradient Methods},
  author={Shevchenko*, Alexander and K{\"o}gler*, Kevin and Hassani, Hamed and Mondelli, Marco},
  journal={arXiv preprint arXiv:2212.13468},
  year={2022},
  selected={true},
  pdf={https://arxiv.org/pdf/2212.13468.pdf}
}

@article{shevchenko2022mean,
  abstract={Understanding the properties of neural networks trained via stochastic gradient descent (SGD) is at the heart of the theory of deep learning. In this work, we take a meanfield view, and consider a two-layer ReLU network trained via noisy-SGD for a univariate regularized regression problem. Our main result is that SGD with vanishingly small noise injected in the gradients is biased towards a simple solution: at convergence, the ReLU network implements a piecewise linear map of the inputs, and the number of “knot” points–ie, points where the tangent of the ReLU network estimator changes–between two consecutive training inputs is at most three. In particular, as the number of neurons of the network grows, the SGD dynamics is captured by the solution of a gradient flow and, at convergence, the distribution of the weights approaches the unique minimizer of a related free energy, which has a Gibbs form. Our key technical contribution consists in the analysis of the estimator resulting from this minimizer: we show that its second derivative vanishes everywhere, except at some specific locations which represent the “knot” points. We also provide empirical evidence that knots at locations distinct from the data points might occur, as predicted by our theory.},
  abbr={JMLR},
  title={Mean-field Analysis of Piecewise Linear Solutions for Wide ReLU Networks},
  author={Shevchenko, Alexander and Kungurtsev, Vyacheslav and Mondelli, Marco},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={130},
  year={2022},
  selected={true},
  pdf={https://www.jmlr.org/papers/volume23/21-1365/21-1365.pdf}
}

@article{shevchenko2020landscape,
  abstract={The optimization of multilayer neural networks typically leads to a solution with zero training error, yet the landscape can exhibit spurious local minima and the minima can be disconnected. In this paper, we shed light on this phenomenon: we show that the combination of stochastic gradient descent (SGD) and over-parameterization makes the landscape of multilayer neural networks approximately connected and thus more favorable to optimization. More specifically, we prove that SGD solutions are connected via a piecewise linear path, and the increase in loss along this path vanishes as the number of neurons grows large. This result is a consequence of the fact that the parameters found by SGD are increasingly dropout stable as the network becomes wider. We show that, if we remove part of the neurons (and suitably rescale the remaining ones), the change in loss is independent of the total number of neurons, and it depends only on how many neurons are left. Our results exhibit a mild dependence on the input dimension: they are dimension-free for two-layer networks and require the number of neurons to scale linearly with the dimension for multilayer networks. We validate our theoretical findings with numerical experiments for different architectures and classification tasks.},
  abbr={ICML},
  title={Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks},
  author={Shevchenko, Alexander and Mondelli, Marco},
  journal={International Conference on Machine Learning},
  year={2020},
  selected={true},
  pdf={http://proceedings.mlr.press/v119/shevchenko20a/shevchenko20a.pdf},
  supp={http://proceedings.mlr.press/v119/shevchenko20a/shevchenko20a-supp.pdf}
>>>>>>> e45564b (initial personal wbp commit)
}
